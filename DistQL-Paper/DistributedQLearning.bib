@article{Tesauro:1995:TDL:203330.203343,
	author = {Tesauro, Gerald},
	title = {Temporal Difference Learning and TD-Gammon},
	journal = {Commun. ACM},
	issue_date = {March 1995},
	volume = {38},
	number = {3},
	month = mar,
	year = {1995},
	issn = {0001-0782},
	pages = {58--68},
	numpages = {11},
	url = {http://doi.acm.org/10.1145/203330.203343},
	doi = {10.1145/203330.203343},
	acmid = {203343},
	publisher = {ACM},
	address = {New York, NY, USA},
}

@InProceedings{A3C,
	title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
	author = 	 {Volodymyr Mnih and Adria Puigdomenech Badia and Mehdi Mirza and Alex Graves and Timothy Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
	pages = 	 {1928--1937},
	year = 	 {2016},
	editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
	volume = 	 {48},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {New York, New York, USA},
	month = 	 {20--22 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
	url = 	 {http://proceedings.mlr.press/v48/mniha16.html},
	abstract = 	 {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
}
 

@article{Gorila,
	author    = {Arun Nair and 
	Praveen Srinivasan and
	Sam Blackwell and
	Cagdas Alcicek and
	Rory Fearon and
	Alessandro De Maria and
	Vedavyas Panneershelvam and
	Mustafa Suleyman and
	Charles Beattie and
	Stig Petersen and
	Shane Legg and
	Volodymyr Mnih and
	Koray Kavukcuoglu and
	David Silver},
	title     = {Massively Parallel Methods for Deep Reinforcement Learning},
	journal   = {CoRR},
	volume    = {abs/1507.04296},
	year      = {2015},
	url       = {http://arxiv.org/abs/1507.04296},
	archivePrefix = {arXiv},
	eprint    = {1507.04296},
	timestamp = {Wed, 07 Jun 2017 14:40:51 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/NairSBAFMPSBPLM15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Mnih2013,
	title = {Playing Atari With Deep Reinforcement Learning},
	author = {Volodymyr Mnih 
	and Koray Kavukcuoglu 
	and David Silver 
	and Alex Graves 
	and Ioannis Antonoglou 
	and Daan Wierstra 
	and Martin Riedmiller},
	booktitle = {NIPS Deep Learning Workshop},
	year = {2013}
}

@Article{Mnih2015,
	author={Mnih, Volodymyr
	and Kavukcuoglu, Koray
	and Silver, David
	and Rusu, Andrei A.
	and Veness, Joel
	and Bellemare, Marc G.
	and Graves, Alex
	and Riedmiller, Martin
	and Fidjeland, Andreas K.
	and Ostrovski, Georg
	and Petersen, Stig
	and Beattie, Charles
	and Sadik, Amir
	and Antonoglou, Ioannis
	and King, Helen
	and Kumaran, Dharshan
	and Wierstra, Daan
	and Legg, Shane
	and Hassabis, Demis},
	title={Human-level control through deep reinforcement learning},
	journal={Nature},
	year={2015},
	month={Feb},
	day={25},
	publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
	volume={518},
	pages={529 EP  -},
	url={http://dx.doi.org/10.1038/nature14236}
}


@phdthesis(watkins,
	author       = {Chris Watkins}, 
	title        = {Learning From Delayed Rewards},
	school       = {Cambridge University},
	year         = 1989,
	address      = {U.K.},
)

@article(qlearning,
	author  = {Watkins, Chris and Dayan, Peter},
	title   = {Q-learning},
	journal = {Machine Learning},
	year    = 1992,
	pages   = {279-292},
	volume  = 8
)

@Article(Tsitsiklis1994,
	author="Tsitsiklis, John N.",
	title="Asynchronous Stochastic Approximation and Q-Learning",
	journal="Machine Learning",
	year="1994",
	month="Sep",
	day="01",
	volume="16",
	number="3",
	pages="185--202",
	abstract="We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.",
	issn="1573-0565",
	doi="10.1023/A:1022689125041",
	url="https://doi.org/10.1023/A:1022689125041"
)

@book(IntrolRL,
	author    = {Richard S. Sutton and Andrew G. Barto}, 
	title     = {Reinforcement Learning: An Introduction},
	publisher = {The MIT Press},
	year      = 1998,
	address   = {Cambridge, MA},
	edition   = 1,
	note      = {An optional note},
	isbn      = {0262193981}
)

@InProceedings(Grounds2008,
	author		="Grounds, Matthew and Kudenko, Daniel",
	editor		="Tuyls, Karl	and Nowe, Ann and Guessoum, Zahia and Kudenko, Daniel",
	title		="Parallel Reinforcement Learning with Linear Function Approximation",
	booktitle	="Adaptive Agents and Multi-Agent Systems III. Adaptation and Multi-Agent Learning",	
	year		="2008",
	publisher	="Springer Berlin Heidelberg",
	address		="Berlin, Heidelberg",
	pages		="60--74",
	abstract	="In this paper, we investigate the use of parallelization in reinforcement learning (RL), with the goal of learning optimal policies for single-agent RL problems more quickly by using parallel hardware. Our approach is based on agents using the SARSA($\lambda$) algorithm, with value functions represented using linear function approximators. In our proposed method, each agent learns independently in a separate simulation of the single-agent problem. The agents periodically exchange information extracted from the weights of their approximators, accelerating convergence towards the optimal policy. We develop three increasingly efficient versions of this approach to parallel RL, and present empirical results for an implementation of the methods on a Beowulf cluster.",
	isbn		="978-3-540-77949-0"
)

@article{MANNION2015956,
	title = "Parallel Reinforcement Learning for Traffic Signal Control",
	journal = "Procedia Computer Science",
	volume = "52",
	pages = "956 - 961",
	year = "2015",
	note = "The 6th International Conference on Ambient Systems, Networks and Technologies (ANT-2015), the 5th International Conference on Sustainable Energy Information Technology (SEIT-2015)",
	issn = "1877-0509",
	doi = "https://doi.org/10.1016/j.procs.2015.05.172",
	url = "http://www.sciencedirect.com/science/article/pii/S1877050915009722",
	author = "Patrick Mannion and Jim Duggan and Enda Howley",
	keywords = "Reinforcement Learning, Parallel Learning, Multi Agent Systems, Intelligent Transportation Systems, Adaptive Traffic Signal Control, Smart Cities"
}

@article{Dietterich2000,
	author = {Dietterich, Thomas G.},
	title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
	journal = {J. Artif. Int. Res.},
	issue_date = {August 2000},
	volume = {13},
	number = {1},
	month = nov,
	year = {2000},
	issn = {1076-9757},
	pages = {227--303},
	numpages = {77},
	url = {http://dl.acm.org/citation.cfm?id=1622262.1622268},
	acmid = {1622268},
	publisher = {AI Access Foundation},
	address = {USA},
} 

@ARTICLE{Barto83, 
	author={A. G. Barto and R. S. Sutton and C. W. Anderson}, 
	journal={IEEE Transactions on Systems, Man, and Cybernetics}, 
	title={Neuronlike adaptive elements that can solve difficult learning control problems}, 
	year={1983}, 
	volume={SMC-13}, 
	number={5}, 
	pages={834-846}, 
	keywords={adaptive control;learning systems;neural nets;adaptive control;adaptive critic element;animal learning studies;associative search element;learning control problem;movable cart;neural nets;neuronlike adaptive elements;Adaptive systems;Biological neural networks;Neurons;Pattern recognition;Problem-solving;Supervised learning;Training}, 
	doi={10.1109/TSMC.1983.6313077}, 
	ISSN={0018-9472}, 
	month={Sept},}

@misc{gym,
	Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	Title = {OpenAI Gym},
	Year = {2016},
	Eprint = {arXiv:1606.01540},
}

@online{Victor:github,
	Author = {VÃ­ctor Mayoral Vilches},
	Title = {Basic Reinforcement Learning Tutorial 4: Q-learning in OpenAI gym},
	Year = {2017},
	URL = {https://github.com/vmayoral/basic_reinforcement_learning/tree/master/tutorial4}
}


