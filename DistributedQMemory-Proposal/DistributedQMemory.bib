@article{Tesauro:1995:TDL:203330.203343,
	author = {Tesauro, Gerald},
	title = {Temporal Difference Learning and TD-Gammon},
	journal = {Commun. ACM},
	issue_date = {March 1995},
	volume = {38},
	number = {3},
	month = mar,
	year = {1995},
	issn = {0001-0782},
	pages = {58--68},
	numpages = {11},
	url = {http://doi.acm.org/10.1145/203330.203343},
	doi = {10.1145/203330.203343},
	acmid = {203343},
	publisher = {ACM},
	address = {New York, NY, USA},
} 

@Article{Mnih2015,
	author={Mnih, Volodymyr
	and Kavukcuoglu, Koray
	and Silver, David
	and Rusu, Andrei A.
	and Veness, Joel
	and Bellemare, Marc G.
	and Graves, Alex
	and Riedmiller, Martin
	and Fidjeland, Andreas K.
	and Ostrovski, Georg
	and Petersen, Stig
	and Beattie, Charles
	and Sadik, Amir
	and Antonoglou, Ioannis
	and King, Helen
	and Kumaran, Dharshan
	and Wierstra, Daan
	and Legg, Shane
	and Hassabis, Demis},
	title={Human-level control through deep reinforcement learning},
	journal={Nature},
	year={2015},
	month={Feb},
	day={25},
	publisher={Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN  -},
	volume={518},
	pages={529 EP  -},
	url={http://dx.doi.org/10.1038/nature14236}
}


@phdthesis(watkins,
	author       = {Chris Watkins}, 
	title        = {Learning From Delayed Rewards},
	school       = {Cambridge University},
	year         = 1989,
	address      = {U.K.},
)

@article(qlearning,
	author  = {Watkins, Chris and Dayan, Peter},
	title   = {Q-learning},
	journal = {Machine Learning},
	year    = 1992,
	pages   = {279-292},
	volume  = 8
)

@Article(Tsitsiklis1994,
	author="Tsitsiklis, John N.",
	title="Asynchronous Stochastic Approximation and Q-Learning",
	journal="Machine Learning",
	year="1994",
	month="Sep",
	day="01",
	volume="16",
	number="3",
	pages="185--202",
	abstract="We provide some general results on the convergence of a class of stochastic approximation algorithms and their parallel and asynchronous variants. We then use these results to study the Q-learning algorithm, a reinforcement learning method for solving Markov decision problems, and establish its convergence under conditions more general than previously available.",
	issn="1573-0565",
	doi="10.1023/A:1022689125041",
	url="https://doi.org/10.1023/A:1022689125041"
)

@book(IntrolRL,
	author    = {Richard S. Sutton and Andrew G. Barto}, 
	title     = {Reinforcement Learning: An Introduction},
	publisher = {The MIT Press},
	year      = 1998,
	address   = {Cambridge, MA},
	edition   = 1,
	note      = {An optional note},
	isbn      = {0262193981}
)

@InProceedings(Grounds,
	author		="Grounds, Matthew and Kudenko, Daniel",
	editor		="Tuyls, Karl	and Nowe, Ann and Guessoum, Zahia and Kudenko, Daniel",
	title		="Parallel Reinforcement Learning with Linear Function Approximation",
	booktitle	="Adaptive Agents and Multi-Agent Systems III. Adaptation and Multi-Agent Learning",	
	year		="2008",
	publisher	="Springer Berlin Heidelberg",
	address		="Berlin, Heidelberg",
	pages		="60--74",
	abstract	="In this paper, we investigate the use of parallelization in reinforcement learning (RL), with the goal of learning optimal policies for single-agent RL problems more quickly by using parallel hardware. Our approach is based on agents using the SARSA($\lambda$) algorithm, with value functions represented using linear function approximators. In our proposed method, each agent learns independently in a separate simulation of the single-agent problem. The agents periodically exchange information extracted from the weights of their approximators, accelerating convergence towards the optimal policy. We develop three increasingly efficient versions of this approach to parallel RL, and present empirical results for an implementation of the methods on a Beowulf cluster.",
	isbn		="978-3-540-77949-0"
)


